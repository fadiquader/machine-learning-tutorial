https://www.youtube.com/watch?v=xYqiIjaqydU&list=PLBv09BD7ez_7-4V3IJIzCHWQj9nd4rVWB

## What is the Naive Bayes Algorithm?
The Naive Bayes Algorithm is a classification technique based on Bayes' theorem. It's called "naive" because it makes a big assumption: it assumes that all features (variables) are independent of each other, which is often not true in real life. Despite this, it works surprisingly well in many situations.

### What does it do?
It predicts the class of an item given its features.
It calculates the probability of each class and picks the one with the highest probability.

---

## How Does It Work?
### 1. Bayes’ Theorem for Classification
Bayes’ theorem is applied like this:

**P(Class | Features) = [P(Features | Class) * P(Class)] / P(Features)**

We want to find:

- **P(Class | Features)**: The probability that the item belongs to a certain class, given its features.
### 2. Simplifying the Formula
Here’s how Naive Bayes simplifies this:

1. **P(Class):** This is the prior probability of the class. It’s how often the class appears in the dataset.
2. **P(Features | Class):** This is the likelihood, or how likely the features are for a given class. The algorithm assumes the features are independent, so this becomes: **P(Features | Class) = P(Feature1 | Class) * P(Feature2 | Class) * ...**
3. **P(Features):** This is the same for all classes, so we don’t need to calculate it when comparing probabilities.
Finally, the formula becomes:

**P(Class | Features) ∝ P(Class) * P(Feature1 | Class) * P(Feature2 | Class) * ...**

The class with the **highest probability** is the predicted class.

---

Steps in the Naive Bayes Algorithm
1. **Calculate prior probabilities for each class**
  - P(Class): Count how many times each class appears in the dataset and divide by the total number of items.
2. **Calculate likelihoods for each feature given the class**
  - For each feature, calculate P(Feature | Class) using the frequency of feature occurrences within that class.
3. Multiply the prior and likelihoods for each class
  - For each class, calculate the total probability:
**P(Class) * P(Feature1 | Class) * P(Feature2 | Class) * ...**
4. Choose the class with the highest probability
  - The class with the largest result is the predicted class.

---

## Example of Naive Bayes
Let’s look at a simple example.

**Problem: Spam Email Classification**

You have a dataset of emails labeled as **Spam** or **Not Spam**. You want to classify a new email as either Spam or Not Spam based on its words.

### Step 1: Dataset

| Email | Word: "Buy" | Word: "Cheap" |  Word: "Win"  | Class |
|-------|-------------|---------------|---------------|-------|
| Email 1 | Yes       | Yes           | No            | Spam
| Email 2 | No        | Yes           | Yes           | Spam
| Email 3 | Yes       | No            | No            | Not Spam
| Email 4 | No        | No            | No            | Not Spam

---

### Step 2: Calculate Priors
How often does each class appear?

- **P(Spam) = 2/4 = 0.5**
- **P(Not Spam) = 2/4 = 0.5**

---
  
### Step 3: Calculate Likelihoods
For each word and class, calculate the probabilities:

| Word       | P(Word / Spam)   | P(Word / Not Spam)       |
|------------|------------------|--------------------------|
| "Buy"      | 1/2 = 0.5        | 1/2 = 0.5       
| "Cheap"    | 2/2 = 1          | 0/2 = 0 
| "Win"      | 1/2 = 0.5        | 0/2 = 0 

---

### Step 4: Predict for a New Email
Suppose we get a new email:
**"Buy Cheap Win"**

We calculate probabilities for both classes:

**For Spam:**

P(Spam | Features) ∝ P(Spam) * P("Buy" | Spam) * P("Cheap" | Spam) * P("Win" | Spam)
= **0.5 * 0.5 * 1 * 0.5 = 0.125**

**For Not Spam:**

P(Not Spam | Features) ∝ P(Not Spam) * P("Buy" | Not Spam) * P("Cheap" | Not Spam) * P("Win" | Not Spam)
= **0.5 * 0.5 * 0 * 0 = 0**

---

### Step 5: Choose the Class with the Highest Probability
- Spam: **0.125**
- Not Spam: **0**
  
The algorithm predicts **Spam** for this email.

---

### Advantages of Naive Bayes
- Simple and Fast: It’s computationally efficient and easy to implement.
- Works Well for Text Data: Especially in spam detection, sentiment analysis, etc.
- Handles Large Datasets: It performs well even with many features.

### Limitations of Naive Bayes
- Assumption of Independence: Features are often not truly independent, which can reduce accuracy.
- Zero Probability Problem: If a feature never appears in the training data for a class, its probability becomes 0, which can mess up the calculations. (This is fixed using Laplace smoothing.)

---

### Summary
Naive Bayes is a classification algorithm based on Bayes' theorem.
- It assumes that features are independent.
- It calculates the probability of each class given the features and picks the one with the highest probability.
- It’s widely used in text classification problems like spam detection and sentiment analysis.
